{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11063799,"sourceType":"datasetVersion","datasetId":6893903},{"sourceId":11373808,"sourceType":"datasetVersion","datasetId":7120430}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-12T05:31:34.810336Z","iopub.execute_input":"2025-04-12T05:31:34.810718Z","iopub.status.idle":"2025-04-12T05:31:38.858962Z","shell.execute_reply.started":"2025-04-12T05:31:34.810691Z","shell.execute_reply":"2025-04-12T05:31:38.857800Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df= pd.read_csv(\"/kaggle/input/audio-files/Dataset/train_labels.csv\")\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T04:31:48.607620Z","iopub.execute_input":"2025-04-12T04:31:48.608101Z","iopub.status.idle":"2025-04-12T04:31:48.657225Z","shell.execute_reply.started":"2025-04-12T04:31:48.608062Z","shell.execute_reply":"2025-04-12T04:31:48.655906Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature Extraction","metadata":{}},{"cell_type":"code","source":"import librosa\n# Extracting information from audios using librosa\n\ntime_series = []\nsample_rate = []\nfor idx in range(1500):\n    audio_path = \"/kaggle/input/audio-files/Dataset/train_folder/\" +str(idx+1) + \".wav\"\n    y, sr = librosa.load(audio_path, sr=None, mono=False)\n    time_series.append(y)\n    sample_rate.append(sr)\n\ndf[\"time_series\"] = time_series\ndf[\"sample_rate\"] = sample_rate\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T04:31:50.806197Z","iopub.execute_input":"2025-04-12T04:31:50.806624Z","iopub.status.idle":"2025-04-12T04:32:15.236527Z","shell.execute_reply.started":"2025-04-12T04:31:50.806586Z","shell.execute_reply":"2025-04-12T04:32:15.235438Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extraction of MFCC features\nn_mfcc = 13\n\nMFCC= []\nDELTA_MFCC= []\nDELTA2_MFCC= []\nMFCC_MEAN= []\nMFCC_VAR= []\nDELTA_MFCC_MEAN= []\nDELTA_MFCC_VAR= []\nDELTA2_MFCC_MEAN= []\nDELTA2_MFCC_VAR= []\n\nfor idx in range(1500):\n    y= df.iloc[idx]['time_series']\n    sr= df.iloc[idx]['sample_rate']\n    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n    MFCC.append(mfcc)\n    \n    delta_mfcc = librosa.feature.delta(mfcc)\n    DELTA_MFCC.append(delta_mfcc)\n    \n    delta2_mfcc = librosa.feature.delta(mfcc, order=2)\n    DELTA2_MFCC.append(delta2_mfcc)\n    \n    mfcc_mean = np.mean(mfcc, axis=1)\n    MFCC_MEAN.append(mfcc_mean)\n    \n    mfcc_var = np.var(mfcc, axis=1)\n    MFCC_VAR.append(mfcc_var)\n    \n    delta_mfcc_mean = np.mean(delta_mfcc, axis=1)\n    DELTA_MFCC_MEAN.append(delta_mfcc_mean)\n    \n    delta_mfcc_var = np.var(delta_mfcc, axis=1)\n    DELTA_MFCC_VAR.append(delta_mfcc_var)\n    \n    delta2_mfcc_mean = np.mean(delta2_mfcc, axis=1)\n    DELTA2_MFCC_MEAN.append(delta2_mfcc_mean)\n    \n    delta2_mfcc_var = np.var(delta2_mfcc, axis=1)\n    DELTA2_MFCC_VAR.append(delta2_mfcc_var)\n\ndf[\"mfcc\"] = MFCC\ndf[\"delta_mfcc\"] = DELTA_MFCC\ndf[\"delta2_mfcc\"] = DELTA2_MFCC\ndf[\"mfcc_mean\"] = MFCC_MEAN\ndf[\"mfcc_var\"] = MFCC_VAR\ndf[\"delta_mfcc_mean\"] = DELTA_MFCC_MEAN\ndf[\"delta_mfcc_var\"] = DELTA_MFCC_VAR\ndf[\"delta2_mfcc_mean\"] = DELTA2_MFCC_MEAN\ndf[\"delta2_mfcc_var\"] = DELTA2_MFCC_VAR\ndf.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T04:32:34.511075Z","iopub.execute_input":"2025-04-12T04:32:34.511423Z","iopub.status.idle":"2025-04-12T04:33:24.003869Z","shell.execute_reply.started":"2025-04-12T04:32:34.511395Z","shell.execute_reply":"2025-04-12T04:33:24.002691Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extraction of 2 time domain features\n\nrms_energy = []\nZCR = []\nfor idx in range(1500):\n    y= df.iloc[idx]['time_series']\n    sr= df.iloc[idx]['sample_rate']\n    rms = librosa.feature.rms(y=y)\n    rms_energy.append(rms)\n    zcr = librosa.feature.zero_crossing_rate(y)\n    ZCR.append(zcr)\n    \ndf[\"rms_energy\"] = rms_energy\ndf[\"zcr\"] = ZCR\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T04:33:31.792111Z","iopub.execute_input":"2025-04-12T04:33:31.792674Z","iopub.status.idle":"2025-04-12T04:34:22.559989Z","shell.execute_reply.started":"2025-04-12T04:33:31.792641Z","shell.execute_reply":"2025-04-12T04:34:22.558386Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extraction of 2 spectral features\n\nspectral_centroid = []\nspectral_bandwidth = []\nfor idx in range(1500):\n    y= df.iloc[idx]['time_series']\n    sr= df.iloc[idx]['sample_rate']\n    s_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n    s_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n    spectral_centroid.append(s_centroid)\n    spectral_bandwidth.append(s_bandwidth)\n    \ndf[\"spectral_centroid\"] = spectral_centroid\ndf[\"spectral_bandwidth\"] = spectral_bandwidth\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T04:34:33.859503Z","iopub.execute_input":"2025-04-12T04:34:33.859901Z","iopub.status.idle":"2025-04-12T04:36:47.466745Z","shell.execute_reply.started":"2025-04-12T04:34:33.859869Z","shell.execute_reply":"2025-04-12T04:36:47.465638Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extraction of 2 rhythmic features\n\nonset_rate = []\npulse_clarity= []\nfor idx in range(1500):\n    y= df.iloc[idx]['time_series']\n    sr= df.iloc[idx]['sample_rate']\n    onset_r = librosa.onset.onset_strength(y=y, sr=sr)\n    onset_rate.append(onset_r)\n    p_clarity= librosa.beat.plp(y=y, sr=sr)\n    pulse_clarity.append(p_clarity)\n    \ndf[\"pulse_clarity\"] = pulse_clarity\ndf[\"onset_rate\"] = onset_rate\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T04:36:51.433257Z","iopub.execute_input":"2025-04-12T04:36:51.433693Z","iopub.status.idle":"2025-04-12T04:38:08.165961Z","shell.execute_reply.started":"2025-04-12T04:36:51.433650Z","shell.execute_reply":"2025-04-12T04:38:08.164758Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df= df.set_index('filename')\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T04:38:19.758850Z","iopub.execute_input":"2025-04-12T04:38:19.759191Z","iopub.status.idle":"2025-04-12T04:38:19.986519Z","shell.execute_reply.started":"2025-04-12T04:38:19.759165Z","shell.execute_reply":"2025-04-12T04:38:19.985309Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"features= list(df.columns)\nfeatures.remove('category')\nfeatures.remove('time_series')\nfeatures.remove('sample_rate')\nfeatures","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T04:38:23.456804Z","iopub.execute_input":"2025-04-12T04:38:23.457158Z","iopub.status.idle":"2025-04-12T04:38:23.464842Z","shell.execute_reply.started":"2025-04-12T04:38:23.457131Z","shell.execute_reply":"2025-04-12T04:38:23.463592Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Normalisation of each of the features by computing mean and variance\n\nimport numpy as np\n\nfor feat in features:\n    mean_col = np.stack(df[feat])\n    mu = np.mean(mean_col, axis=0)\n    std = np.std(mean_col, axis=0)\n    \n    mean_col = (mean_col - mu) / std\n    mean_col= np.nan_to_num(mean_col, nan= 0)\n    df[feat+\"_normalised\"] = mean_col.tolist()\n    \n\ndf.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T04:38:25.966775Z","iopub.execute_input":"2025-04-12T04:38:25.967123Z","iopub.status.idle":"2025-04-12T04:38:28.975964Z","shell.execute_reply.started":"2025-04-12T04:38:25.967097Z","shell.execute_reply":"2025-04-12T04:38:28.974871Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"# Make plots for other features that have been computed (Waveform, ZCR and spectrogram)\n\nimport librosa.display\nimport matplotlib.pyplot as plt\n\ni= int(input(\"Which datapoint's plots are required? \"))\ny= df.iloc[i]['time_series']\nzcr= df.iloc[i]['zcr']\nsr= df.iloc[i]['sample_rate']\n\ntime = np.linspace(0, len(y) / sr, num=len(y))\n\nframes = range(len(zcr[0]))\ntime_zcr = librosa.frames_to_time(frames, sr=sr, hop_length=512)\n\nplt.figure(figsize=(12, 6))\n\n# Plot waveform\nplt.subplot(2, 1, 1)\nplt.plot(time, y, label=\"Waveform\", color=\"blue\", alpha=0.7)\nplt.xlabel(\"Time (s)\")\nplt.ylabel(\"Amplitude\")\nplt.title(\"Waveform of the Audio Signal\")\nplt.legend()\nplt.tight_layout()\n\n# Plot Zero Crossing Rate\nplt.subplot(2, 1, 2)\nplt.plot(time_zcr, zcr[0], label=\"Zero Crossing Rate\", color=\"red\")\nplt.xlabel(\"Time (s)\")\nplt.ylabel(\"ZCR\")\nplt.title(\"Zero Crossing Rate Over Time\")\nplt.legend()\nplt.tight_layout()\n\n\nD = librosa.amplitude_to_db(abs(librosa.stft(y)), ref=np.max)\n\n# Plot the spectrogram\nplt.figure(figsize=(10, 3))\nlibrosa.display.specshow(D, sr=sr, x_axis='time', y_axis='log')  \nplt.colorbar(format='%+2.0f dB')\nplt.title('Spectrogram')\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T04:45:49.681685Z","iopub.execute_input":"2025-04-12T04:45:49.682052Z","iopub.status.idle":"2025-04-12T04:45:54.037745Z","shell.execute_reply.started":"2025-04-12T04:45:49.682026Z","shell.execute_reply":"2025-04-12T04:45:54.036436Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Separating multidimensional features into individual columns, followed by normalisation\nimport numpy as np\nimport pandas as pd\n\n# Creating separate lists for 1D and 2D arrays, to address them accordingly\nfeatures_1d = ['mfcc_mean_normalised', 'mfcc_var_normalised',\n       'delta_mfcc_mean_normalised', 'delta_mfcc_var_normalised',\n       'delta2_mfcc_mean_normalised', 'delta2_mfcc_var_normalised',\n        'pulse_clarity_normalised', 'onset_rate_normalised']\nfeatures_2d = ['mfcc_normalised', 'delta_mfcc_normalised',\n       'delta2_mfcc_normalised', 'zcr_normalised',\n        'rms_energy_normalised','spectral_centroid_normalised', 'spectral_bandwidth_normalised']\n\ndf_exp = {}\n\nfor feat in features_1d:\n    arr = np.vstack(df[feat].values)\n    for i in range(arr.shape[1]):\n        df_exp[f'{feat}_{i}'] = arr[:, i] \n\nfor feat in features_2d:\n    arr = np.array(df[feat].tolist())  \n    reshaped_arr = arr.reshape(arr.shape[0], -1)  \n    col_names = [f'{feat}_{i}_{j}' for i in range(arr.shape[1]) for j in range(arr.shape[2])]\n    \n    for j, col in enumerate(col_names):\n        df_exp[col] = reshaped_arr[:, j]  \n\n\ndf_exp = pd.DataFrame(df_exp)\n\ndf_exp.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T04:57:27.421381Z","iopub.execute_input":"2025-04-12T04:57:27.421857Z","iopub.status.idle":"2025-04-12T04:57:29.672714Z","shell.execute_reply.started":"2025-04-12T04:57:27.421808Z","shell.execute_reply":"2025-04-12T04:57:29.671676Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generating numerical equivalents for the class labels\n\nfrom sklearn.preprocessing import OrdinalEncoder\nencoder = OrdinalEncoder()\n\ndf['category'] = encoder.fit_transform(df[['category']])\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T04:49:55.782458Z","iopub.execute_input":"2025-04-12T04:49:55.782865Z","iopub.status.idle":"2025-04-12T04:49:56.107508Z","shell.execute_reply.started":"2025-04-12T04:49:55.782834Z","shell.execute_reply":"2025-04-12T04:49:56.106321Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculating correlations for individual feature columns (normalised) against class labels\n\nfrom tqdm import tqdm\nfrom scipy.stats import pearsonr\n\ncorrelations= []\nlabel_cols = df_exp.columns \nfor label_col in tqdm(label_cols, desc=\"Plotting for Label Columns\"):\n    corr, p_value = pearsonr(df_exp[label_col], df['category'])\n    correlations.append(corr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T19:34:03.130129Z","iopub.execute_input":"2025-04-09T19:34:03.130509Z","iopub.status.idle":"2025-04-09T19:34:26.377887Z","shell.execute_reply.started":"2025-04-09T19:34:03.130474Z","shell.execute_reply":"2025-04-09T19:34:26.377042Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plotting individual columns from the same feature on the same graph\n\nfor feat in features_1d:\n    l= len(feat)\n    y_vals= []\n    x_vals= []\n    for i in range(len(label_cols)):\n        col= label_cols[i]\n        if col[0:l]== feat:\n            y_vals.append(correlations[i])\n    plt.figure(figsize=(8, 5))\n    x_vals= list(range(1, len(y_vals)+1))\n    plt.barh(x_vals, y_vals, color='skyblue')\n    plt.xlabel('Correlation')\n    plt.title('Correlation with ' + feat)\n    plt.axvline(0, color='gray', linestyle='--')\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T12:56:47.470025Z","iopub.execute_input":"2025-04-08T12:56:47.470425Z","iopub.status.idle":"2025-04-08T12:56:50.623490Z","shell.execute_reply.started":"2025-04-08T12:56:47.470394Z","shell.execute_reply":"2025-04-08T12:56:50.622427Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for feat in features_2d:\n    l= len(feat)\n    y_vals= []\n    x_vals= []\n    for i in range(len(label_cols)):\n        col= label_cols[i]\n        if col[0:l]== feat:\n            y_vals.append(correlations[i])\n    plt.figure(figsize=(8, 5))\n    x_vals= list(range(1, len(y_vals)+1))\n    plt.barh(x_vals, y_vals, color='skyblue')\n    plt.xlabel('Correlation')\n    plt.title('Correlation with '+ feat)\n    plt.axvline(0, color='gray', linestyle='--')\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T13:04:33.395367Z","iopub.execute_input":"2025-04-08T13:04:33.395738Z","iopub.status.idle":"2025-04-08T13:05:01.902714Z","shell.execute_reply.started":"2025-04-08T13:04:33.395710Z","shell.execute_reply":"2025-04-08T13:05:01.900733Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_exp['category']= np.array(df['category'])\ndf_exp.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T04:58:28.818439Z","iopub.execute_input":"2025-04-12T04:58:28.818851Z","iopub.status.idle":"2025-04-12T04:58:28.846608Z","shell.execute_reply.started":"2025-04-12T04:58:28.818818Z","shell.execute_reply":"2025-04-12T04:58:28.845413Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Storing the processed feature CSV, to be used later for PCA\ndf_exp.to_csv('pre_pca_features.csv', index= False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T05:14:01.090989Z","iopub.execute_input":"2025-04-12T05:14:01.091414Z","iopub.status.idle":"2025-04-12T05:14:58.919064Z","shell.execute_reply.started":"2025-04-12T05:14:01.091383Z","shell.execute_reply":"2025-04-12T05:14:58.917927Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plotting feature distributions vs encoded class labels\n\nprint(df_exp.columns)\nfeature = input(\"What feature distribution do you want to plot?\")\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_theme(style=\"whitegrid\")\n\nplt.figure(figsize=(10, 6))\n\nsns.scatterplot(x=feature, y='category', data=df_exp, color='royalblue')\n\nplt.title(feature, fontsize=16, fontweight='bold')\nplt.xlabel(feature + \" values\", fontsize=12)\nplt.ylabel(\"Category\", fontsize=12)\n\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=10)\nplt.grid(True, linestyle='--', alpha=0.6)\n\nsns.despine()\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T13:26:34.382924Z","iopub.execute_input":"2025-04-08T13:26:34.383354Z","iopub.status.idle":"2025-04-08T13:26:37.380286Z","shell.execute_reply.started":"2025-04-08T13:26:34.383322Z","shell.execute_reply":"2025-04-08T13:26:37.379113Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Loading features from the saved CSV file\ndf_exp_n= pd.read_csv(\"/kaggle/input/pre-pca-features/pre_pca_features.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T05:34:41.605360Z","iopub.execute_input":"2025-04-12T05:34:41.605972Z","iopub.status.idle":"2025-04-12T05:34:56.643824Z","shell.execute_reply.started":"2025-04-12T05:34:41.605932Z","shell.execute_reply":"2025-04-12T05:34:56.642652Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dimensionality Reduction & Train-Test-Val Split","metadata":{}},{"cell_type":"code","source":"df_exp_n= df_exp_n.drop(['Unnamed: 0'], axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T05:35:26.228919Z","iopub.execute_input":"2025-04-12T05:35:26.229268Z","iopub.status.idle":"2025-04-12T05:35:26.334620Z","shell.execute_reply.started":"2025-04-12T05:35:26.229242Z","shell.execute_reply":"2025-04-12T05:35:26.333667Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\n\n# Perform PCA, while retaining 95% of the total variance\npca = PCA(n_components=0.95)\ndf_pca = pca.fit_transform(df_exp_n.drop(['category'], axis=1))\n\ndf_pca = pd.DataFrame(df_pca, columns=[f'PC{i+1}' for i in range(df_pca.shape[1])])\n\nexplained_variance = pca.explained_variance_ratio_\n\ndf_pca.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T05:35:35.484854Z","iopub.execute_input":"2025-04-12T05:35:35.485203Z","iopub.status.idle":"2025-04-12T05:36:03.845262Z","shell.execute_reply.started":"2025-04-12T05:35:35.485175Z","shell.execute_reply":"2025-04-12T05:36:03.843511Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_pca_50 = df_pca.iloc[:, :50]  # Select first 50 columns\ndf_pca_50.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T05:36:33.334065Z","iopub.execute_input":"2025-04-12T05:36:33.334487Z","iopub.status.idle":"2025-04-12T05:36:33.357808Z","shell.execute_reply.started":"2025-04-12T05:36:33.334451Z","shell.execute_reply":"2025-04-12T05:36:33.356698Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot of explained variance vs the no. of components taken\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nplt.figure(figsize=(8,5))\nplt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o', linestyle='--')\nplt.xlabel('Number of Principal Components')\nplt.ylabel('Cumulative Explained Variance')\nplt.title('Explained Variance vs. Number of Components')\nplt.grid()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T05:36:37.031903Z","iopub.execute_input":"2025-04-12T05:36:37.032285Z","iopub.status.idle":"2025-04-12T05:36:37.250108Z","shell.execute_reply.started":"2025-04-12T05:36:37.032256Z","shell.execute_reply":"2025-04-12T05:36:37.248890Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generating train, test and validation splits\n\nfrom sklearn.model_selection import train_test_split\n\ntrain_df, temp_df = train_test_split(df_pca_50, test_size=0.4, shuffle=False)\nval_df, test_df = train_test_split(temp_df, test_size=0.5, shuffle = False)\n \ntrain_label, temp_label = train_test_split(df['category'], test_size=0.4, shuffle=False)\nval_label, test_label = train_test_split(temp_label, test_size=0.5, shuffle = False)\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T05:01:03.019353Z","iopub.execute_input":"2025-04-12T05:01:03.019747Z","iopub.status.idle":"2025-04-12T05:01:03.244174Z","shell.execute_reply.started":"2025-04-12T05:01:03.019716Z","shell.execute_reply":"2025-04-12T05:01:03.243144Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Self-implemented K-Means clustering","metadata":{}},{"cell_type":"code","source":"# Our implementation of K-Means clustering\n\nimport numpy as np\nimport pandas as pd\n\nclass DIY_KMeans:\n    def __init__(self, k, max_iters=100, tol=1e-4):\n        self.k = k\n        self.max_iters = max_iters\n        self.tol = tol\n        self.centroids = None\n\n    # Function to obtain cluster centres\n    def fit(self, X: pd.DataFrame):\n        n_samples = X.shape[0]\n        random_indices = np.random.choice(n_samples, self.k, replace=False)\n        self.centroids = X.iloc[random_indices].copy().reset_index(drop=True)\n        \n        for _ in range(self.max_iters):\n            labels = self._assign_clusters(X)\n            \n            new_centroids = X.groupby(labels).mean()\n            \n            if new_centroids.shape[0] < self.k:\n                break\n            \n            if np.linalg.norm(self.centroids.values - new_centroids.values) < self.tol:\n                break\n            \n            self.centroids = new_centroids.reset_index(drop=True)\n\n    # Function to assign clusters to all data points\n    def _assign_clusters(self, X: pd.DataFrame):\n        distances = np.linalg.norm(X.values[:, np.newaxis] - self.centroids.values, axis=2)\n        return np.argmin(distances, axis=1)\n\n    # Function to generate predictions on the test set\n    def predict(self, X: pd.DataFrame):\n        return self._assign_clusters(X)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T05:01:07.616632Z","iopub.execute_input":"2025-04-12T05:01:07.616987Z","iopub.status.idle":"2025-04-12T05:01:07.625126Z","shell.execute_reply.started":"2025-04-12T05:01:07.616962Z","shell.execute_reply":"2025-04-12T05:01:07.623912Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import adjusted_rand_score\n\nkmeans = DIY_KMeans(k=50)\nkmeans.fit(train_df)\n\ntrain_pred= kmeans.predict(train_df) \nval_pred = kmeans.predict(val_df)\ntest_pred = kmeans.predict(test_df)\n\nari_train_score = adjusted_rand_score(train_label,train_pred)\nari_val_score = adjusted_rand_score(val_label,val_pred)\nari_test_score = adjusted_rand_score(test_label,test_pred)\n\nprint(f\"Train ARI Score: {ari_train_score}\")\nprint(f\"Validation ARI Score: {ari_val_score}\")\nprint(f\"Test ARI Score: {ari_test_score}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T05:01:12.933140Z","iopub.execute_input":"2025-04-12T05:01:12.933486Z","iopub.status.idle":"2025-04-12T05:01:13.128477Z","shell.execute_reply.started":"2025-04-12T05:01:12.933459Z","shell.execute_reply":"2025-04-12T05:01:13.127267Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## K-Means and DBSCAN using inbuilt libraries","metadata":{}},{"cell_type":"code","source":"# K-Means using inbuilt libraries\nfrom sklearn.cluster import KMeans\n\nkmeans2 = KMeans(n_clusters=50, random_state=42, n_init=10)\nkmeans2.fit(train_df)\n\ntrain_pred2= kmeans2.predict(train_df) \nval_pred2 = kmeans2.predict(val_df)\ntest_pred2 = kmeans2.predict(test_df)\n\nari_train_score2 = adjusted_rand_score(train_label,train_pred2)\nari_val_score2 = adjusted_rand_score(val_label,val_pred2)\nari_test_score2 = adjusted_rand_score(test_label,test_pred2)\n\nprint(f\"Train ARI Score: {ari_train_score2}\")\nprint(f\"Validation ARI Score: {ari_val_score2}\")\nprint(f\"Test ARI Score: {ari_test_score2}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T05:01:21.224608Z","iopub.execute_input":"2025-04-12T05:01:21.225007Z","iopub.status.idle":"2025-04-12T05:01:21.431849Z","shell.execute_reply.started":"2025-04-12T05:01:21.224966Z","shell.execute_reply":"2025-04-12T05:01:21.430844Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.cluster import DBSCAN\nfrom sklearn.metrics import adjusted_rand_score\nfrom tqdm import tqdm\n\n# Hyperparameter tuning for DBSCAN (choosing eps and min_samples)\n\neps_values = np.arange(1, 100, 1)\nmin_samples_values = range(3, 250)\n\nbest_score = -1\nbest_params = {}\n\ntrue_labels = val_label\n\nfor eps in tqdm(eps_values, desc=\"Tuning DBSCAN\"):\n    for min_samples in min_samples_values:\n        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n        labels = dbscan.fit_predict(val_df)\n\n        if len(set(labels)) > 1 and len(set(labels)) < len(val_df):\n            score = adjusted_rand_score(true_labels, labels)\n            if score > best_score:\n                best_score = score\n                best_params = {\"eps\": eps, \"min_samples\": min_samples}\n\nprint(f\"Best Params: {best_params}, Best ARI Score: {best_score:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T05:01:52.381771Z","iopub.execute_input":"2025-04-12T05:01:52.382131Z","iopub.status.idle":"2025-04-12T05:04:07.618762Z","shell.execute_reply.started":"2025-04-12T05:01:52.382103Z","shell.execute_reply":"2025-04-12T05:04:07.617160Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# DBSCAN using inbuilt libraries\n# Best Params: {'eps': 49, 'min_samples': 3}, Best ARI Score: 0.0291\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.metrics import adjusted_rand_score\n\ndbscan = DBSCAN(eps=49, min_samples=3)\ntrain_pred3 = dbscan.fit_predict(train_df)\nval_pred3 = dbscan.fit_predict(val_df)\ntest_pred3 = dbscan.fit_predict(test_df)\n\nari_train_score3 = adjusted_rand_score(train_label,train_pred3)\nari_val_score3 = adjusted_rand_score(val_label,val_pred3)\nari_test_score3 = adjusted_rand_score(test_label,test_pred3)\n\nprint(f\"Train ARI Score: {ari_train_score3}\")\nprint(f\"Validation ARI Score: {ari_val_score3}\")\nprint(f\"Test ARI Score: {ari_test_score3}\")","metadata":{"execution":{"iopub.status.busy":"2025-04-12T05:04:36.403176Z","iopub.execute_input":"2025-04-12T05:04:36.403526Z","iopub.status.idle":"2025-04-12T05:04:36.436710Z","shell.execute_reply.started":"2025-04-12T05:04:36.403500Z","shell.execute_reply":"2025-04-12T05:04:36.435880Z"},"trusted":true},"outputs":[],"execution_count":null}]}